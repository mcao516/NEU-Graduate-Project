{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_ids: [batch_size, max_seq_len, max_word_len]\n",
    "char_ids = torch.randint(10, (1, 2, 3), dtype=torch.long)\n",
    "batch_size, max_seq_len, max_word_len = char_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[9, 5, 7],\n",
      "         [0, 5, 2]]])\n"
     ]
    }
   ],
   "source": [
    "print(char_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.3530, -0.1975, -0.4178, -0.0190],\n",
      "          [ 0.4614,  1.0711, -1.7990, -0.3324],\n",
      "          [-1.3839, -0.0398, -0.7629, -0.3294]],\n",
      "\n",
      "         [[ 1.3249,  2.1199,  1.2087, -1.2470],\n",
      "          [ 0.4614,  1.0711, -1.7990, -0.3324],\n",
      "          [-0.7052,  1.6807,  0.2693, -2.2881]]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "char_dim = 4\n",
    "char_embed = nn.Embedding(10, char_dim)\n",
    "char_embedded = char_embed(char_ids)\n",
    "print(char_embedded.shape) # [batch_size, max_seq_len, max_word_len, char_embedding_dim]\n",
    "print(char_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3])\n",
      "tensor([[[-0.3530,  0.4614, -1.3839],\n",
      "         [-0.1975,  1.0711, -0.0398],\n",
      "         [-0.4178, -1.7990, -0.7629],\n",
      "         [-0.0190, -0.3324, -0.3294]],\n",
      "\n",
      "        [[ 1.3249,  0.4614, -0.7052],\n",
      "         [ 2.1199,  1.0711,  1.6807],\n",
      "         [ 1.2087, -1.7990,  0.2693],\n",
      "         [-1.2470, -0.3324, -2.2881]]], grad_fn=<PermuteBackward>)\n"
     ]
    }
   ],
   "source": [
    "char_embedded = char_embedded.view(-1, max_word_len, char_dim)\n",
    "char_embedded = char_embedded.permute(0, 2, 1)\n",
    "print(char_embedded.shape) # [batch_size*max_seq_len, char_embedding_dim, max_word_len]\n",
    "print(char_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter\n",
    "conv1 = nn.Conv1d(char_dim, 3, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1])\n",
      "tensor([[[-0.1603],\n",
      "         [-0.9347],\n",
      "         [ 0.9406]],\n",
      "\n",
      "        [[ 1.2844],\n",
      "         [-0.8436],\n",
      "         [ 0.9502]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "conv_out = conv1(char_embedded)\n",
    "print(conv_out.shape) # [batch_size*max_seq_len, out_channels, max_word_len + 1 - kernel_size]\n",
    "print(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "output = F.max_pool2d(conv_out, kernel_size=(1, conv_out.shape[-1])).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([[-0.1603, -0.9347,  0.9406],\n",
      "        [ 1.2844, -0.8436,  0.9502]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(output.shape) # [batch_size*max_seq_len, out_channels]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayMLP(nn.Module):\n",
    "    \"\"\"Implement highway network.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 activation=nn.functional.relu,\n",
    "                 gate_activation=torch.sigmoid):\n",
    "        \n",
    "        super(HighwayMLP, self).__init__()\n",
    "        \n",
    "        self.act, self.gate_act = activation, gate_activation\n",
    "        \n",
    "        self.mlp = nn.Linear(input_size, input_size)\n",
    "        self.transform = nn.Linear(input_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [*, input_size]\n",
    "        \"\"\"\n",
    "        mlp_out = self.act(self.mlp(x))\n",
    "        gate_out = self.gate_act(self.transform(x))\n",
    "        \n",
    "        return gate_out * mlp_out + (1 - gate_out) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNN(nn.Module):\n",
    "    \"\"\"Character-level embedding with convolutional neural networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 char_size,\n",
    "                 char_dim,\n",
    "                 filter_num=10,\n",
    "                 max_filter_size=7,\n",
    "                 output_size=50,\n",
    "                 padding_idx=0,\n",
    "                 dropout=0.2):\n",
    "        \"\"\"Constructs CharCNN model.\n",
    "        \n",
    "        Args:\n",
    "            char_size: total characters in the vocabulary.\n",
    "            char_dim: character embedding size.\n",
    "            filter_num: number of filters (each size).\n",
    "            dropout: dropout rate.\n",
    "        \"\"\"\n",
    "        super(CharCNN, self).__init__()\n",
    "        \n",
    "        self.char_dim = char_dim\n",
    "        self.filter_num = filter_num\n",
    "        self.max_filter_size = max_filter_size\n",
    "        \n",
    "        self.embed = nn.Embedding(char_size, \n",
    "                                  char_dim, \n",
    "                                  padding_idx=padding_idx)\n",
    "        \n",
    "        self.filters = nn.ModuleList()\n",
    "        for size in range(1, max_filter_size + 1):\n",
    "            self.filters.append(nn.Conv1d(char_dim, \n",
    "                                          filter_num, \n",
    "                                          kernel_size=size))\n",
    "        \n",
    "        self.highway = HighwayMLP(max_filter_size * filter_num)\n",
    "        self.proj = nn.Linear(max_filter_size * filter_num, output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, char_ids, word_lens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            char_ids: [batch_size, max_seq_len, max_word_len]\n",
    "            word_lens: [batch_size, max_seq_len]\n",
    "\n",
    "        Return:\n",
    "            c_emb: [batch_size, max_seq_len, char_hidden_dim]\n",
    "        \"\"\"\n",
    "        batch_size, max_seq_len, max_word_len = char_ids.shape\n",
    "        \n",
    "        # embedding\n",
    "        char_embedded = self.embed(char_ids)\n",
    "        char_embedded = char_embedded.view(-1, max_word_len, self.char_dim)\n",
    "        char_embedded = char_embedded.permute(0, 2, 1) # [batch_size*max_seq_len, char_dim, max_word_len]\n",
    "        \n",
    "        # convolution layer & max pooling\n",
    "        outputs = []\n",
    "        for i, conv in enumerate(self.filters):\n",
    "            if i + 1 <= max_word_len:\n",
    "                conv_out = conv(char_embedded)\n",
    "                out = F.max_pool2d(conv_out, kernel_size=(1, conv_out.shape[-1]))\n",
    "                out = out.squeeze(-1) # [batch_size*max_seq_len, filter_num]\n",
    "                outputs.append(out)\n",
    "        \n",
    "        outputs = torch.cat(outputs, -1)\n",
    "        outputs = self._pad_outputs(outputs)\n",
    "        assert outputs.shape == (batch_size * max_seq_len, self.max_filter_size * self.filter_num)\n",
    "        \n",
    "        # highway network\n",
    "        highway_out = self.highway(outputs)\n",
    "        \n",
    "        # proj\n",
    "        final_out = torch.relu(self.proj(highway_out))\n",
    "        final_out = final_out.view(batch_size, max_seq_len, -1)\n",
    "        \n",
    "        return final_out\n",
    "    \n",
    "    \n",
    "    def _pad_outputs(self, x):\n",
    "        \"\"\"In case the max word length is less than the max filter width, \n",
    "           use this function to pad the output.\n",
    "           \n",
    "        Args:\n",
    "            x: tensor, [batch_size * max_seq_len, N * filter_num] (N <= max_filter_size)\n",
    "\n",
    "        return:\n",
    "            out: tensor, [batch_size * max_seq_len, max_filter_size * filter_num]\n",
    "        \"\"\"\n",
    "        bm, input_size = x.shape\n",
    "        dim_to_pad = self.filter_num * self.max_filter_size - input_size\n",
    "        assert dim_to_pad >= 0\n",
    "        \n",
    "        if dim_to_pad == 0:\n",
    "            return x\n",
    "        else: \n",
    "            padder = torch.zeros((bm, dim_to_pad), dtype=x.dtype, device=x.device)\n",
    "            out = torch.cat((x, padder), -1)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "charCNN = CharCNN(char_size=100,\n",
    "                  char_dim=20,\n",
    "                  filter_num=10,\n",
    "                  max_filter_size=7,\n",
    "                  output_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_ids = torch.randint(100, [32, 20, 10], dtype=torch.int64)\n",
    "out = charCNN(char_ids, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 50])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
